---
title: Http1.x/Http2.x
order: 1
group:
  order: 2
  title: 网络
  path: /interview/network
nav:
  order: 3
  title: 'interview'
  path: /interview
---

## 本章需要了解的的知识点

## 七层网络模型/四层 TCP/IP

**OSI**(Open Systems Interconncection，开放系统互联)网络分层,从上到下依次是：

7.应用层(Application) // Http/ssl

6.表示层(Presentation)

5.会话层(Session)

4.传输层(Transport) // tcp/udp

3.网络层(Network) -- 路由器

2.数据链路层(Data Link) -- 交换机

1.物理层(Physical) -- 网卡、集线器（Hub）

## http1.x

### 性能缺陷

1.高延迟：页面访问速度下降

虽然近几年来网络带宽增长非常快，然而我们却并没有看到网络延迟有对应程度的降低，这主要是由于队头阻塞 (Head-Of-Line Blocking)问题导致

HTTP/1.1 版引入了管道机制（pipelining），即在同一个 TCP 连接里面，客户端可以同时发送多个请求，进一步改进了 HTTP 协议的效率但这要求服务端必须按照请求发送的顺序返回响应，当顺序请求多个文件时，其中一个请求因为某种原因被阻塞时，在后面排队的所有请求也一并被阻塞，这就是队头阻塞 (Head-Of-Line Blocking)

因此 人们尝试过以下办法来解决队头阻塞问题：

1.1 使用多个域名 ：将同一个页面的资源分散到不同域名，提升并发连接上限，因为浏览器通常对同一域名的 HTTP 连接最大只能是 6 个引入雪碧图 ：将多张小图合并成一张大图供浏览器 JavaScript 来切割使用，这样可以将多个请求合并成一个请求，但是带来了新的问题，当某张小图片更新了，那么需要重新请求大图片，浪费了大量的网络带宽； 1.2.将小图内联 ：将图片的二进制数据通过 base64 编码后，把编码数据嵌入到 HTML 或 CSS 文件中，以此来减少网络请求次数； 1.3 按需加载 ：来减少第一时间的 HTTP 请求次数

2. 明文传输：不安全 HTTP/1.1 在传输数据时，所有传输的内容都是明文，客户端和服务器端都无法验证对方的身份，这在一定程度上无法保证数据的安全性。

3. 无状态：头部巨大切重复由于 HTTP 协议是无状态的，每一个请求都得携带 HTTP 头部，特别是对于有携带 cookie 的头部，而 cookie 的大小通常很大，另外还有 User Agent、Accept、Server 等，通常多达几百字节甚至上千字节，但 Body 却经常只有几十字节

4. 不支持服务器推送 HTTP/1.1 不支持服务器推送消息，因此当客户端需要获取通知时，只能通过定时器不断地拉取消息，这无疑浪费大量了带宽和服务器资源。

## http2.0

### 多路复用

简单版回答：多路复用

- 同一域名下，只需要建立一个连接。 => 减少握手等待时间，以及多个 tcp 竞争带宽。
- 单个连接可以承受任意数量的双向数据流。 => 并行多个请求响应。
- 数据流以消息的形式发送，消息由一个或多个帧组成；帧可以乱序发送，根据帧头部的流标识重新组装。 => 可以设置一个 31 bit 的优先级，有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。

> 在 HTTP/2 中，有两个非常重要的概念，分别是帧（frame）和流（stream）。帧代表着最小的数据单位，每个帧会标识出该帧属于哪个流，流也就是多个帧组成的数据流。多路复用，**就是在一个 TCP 连接中可以存在多条流**。换句话说，也就是可以发送多个请求，对端可以通过帧中的标识知道属于哪个请求。通过这个技术，可以避免 HTTP 旧版本中的队头阻塞问题，极大的提高传输性能

在 HTTP/2 中引入了多路复用的技术。多路复用很好地解决了浏览器限制同一个域名下请求数量的问题，同时也更容易实现全速传输，毕竟新开一个 TCP 连接都需要慢慢提升传输速度。

这一特性使得 HTTP 传输性能得到极大提升

- 多工 HTTP/2 复用 TCP 连接，在一个连接里，客户端和浏览器都可以同时发送多个请求或回应，而且不用按照顺序一一对应，这样就避免了"队头堵塞" ![http2复用 TCP 连接](https://s2.loli.net/2022/04/30/rnqxl3Lvd6WCFcK.jpg)

- 数据流 HTTP/2 并行交错地发送多个请求 / 响应，请求 / 响应之间互不影响,为此每个数据包都会做标记，是请求还是响应；

![http2数据流](https://s2.loli.net/2022/04/30/kOweT46pM2rcudq.jpg)

HTTP/2 将每个请求或回应的所有数据包，称为一个数据流（stream）。每个数据流都有一个独一无二的编号。数据包发送的时候，都必须标记数据流 ID，用来区分它属于哪个数据流。另外还规定，客户端发出的数据流，ID 一律为奇数，服务器发出的，ID 为偶数。数据流发送到一半的时候，客户端和服务器都可以发送信号（RST_STREAM 帧），取消这个数据流。1.1 版取消数据流的唯一方法，就是关闭 TCP 连接。这就是说，HTTP/2 可以取消某一次请求，同时保证 TCP 连接还打开着，可以被其他请求使用。

- 优先级

在 HTTP/2 中，每个请求都可以带一个 31bit 的优先值，0 表示最高优先级， 数值越大优先级越低。有了这个优先值，客户端和服务器就可以在处理不同的流时采取不同的策略，以最优的方式发送流、消息和帧。

### 二进制帧/二进制传输

关键之一就是在应用层(HTTP/2)和传输层(TCP or UDP)之间增加一个二进制分帧层，在二进制分帧层中， HTTP/2 会将所有传输的信息分割为更小的消息和帧（frame），并对它们采用二进制格式的编码。HTTP/2 中，同域名下所有通信都在**单个连接上**完成，该连接可以承载任意数量的双向数据流。每个数据流都以消息的形式发送，而消息又由一个或多个帧组成。多个帧之间可以乱序发送，根据帧首部的流标识可以重新组装

### 服务端推送

HTTP/2 允许服务器未经请求，主动向客户端发送资源，这叫做服务器推送（server push）。

常见场景是客户端请求一个网页，这个网页里面包含很多静态资源。正常情况下，客户端必须收到网页后，解析 HTML 源码，发现有静态资源，再发出静态资源请求。其实，服务器可以预期到客户端请求网页后，很可能会再请求静态资源，所以就主动把这些静态资源随着网页一起发给客户端了。

这样就可以相对减少一点延迟时间。当然在浏览器兼容的情况下你也可以使用 **prefetch** 。

> 注意： 服务端可以主动推送，客户端也可以主动选择是否接收，如果服务端推送的资源已经被浏览器缓存过，浏览器可以通过发送 RST_STREAM 帧来拒收，另外，主动推送也遵守同源策略

> proload 资源预加载，prefetch 在**空闲时加载可能会利用到的资源**.

### header compression(HPACK)

HTTP 协议不带有状态，每次请求都必须附上所有信息。所以，请求的很多字段都是重复的，比如 Cookie 和 User Agent，一模一样的内容，每次请求都必须附带，这会浪费很多带宽，也影响速度。

HTTP/2 对这一点做了优化，引入了头信息压缩机制（header compression）。一方面，头信息使用 gzip 或 compress 压缩后再发送；另一方面，客户端和服务器同时维护一张头信息表，所有字段都会存入这个表，生成一个索引号，以后就不发送同样字段了，只发送索引号，这样就提高速度了

### 安全性提高

出于兼容的考虑，HTTP/2 延续了 HTTP/1 的“明文”特点，可以像以前一样使用明文传输数据，不强制使用加密通信，但 HTTPS 已经是大势所趋，各大主流浏览器都公开宣布只支持加密的 HTTP/2，所以，真实应用中的 HTTP/2 是还是加密的

## HTTP/2 下还会拥塞吗？

- 由于 TCP 连接减少而使网络拥塞状况得以改观
- 慢启动时间减少，拥塞和丢包恢复速度更快。

## Http2.0 遗留问题

- HTTP/2 还会队头阻塞吗？ HTTP/2 也存在队头阻塞问题，比如**丢包**。如果造成队头阻塞，问题可能比 http1.1 还严重，因为只有一个 tcp 连接，后续的传输都要等前面，http/1.1 多个 tcp 连接，阻塞一个，其他的还可以正常跑

## Http3.0

[Http3.0](./http3.md)

## 参考

- [HTTP/2 对比 HTTP/1.1，新特性是什么？是如何解决队头阻塞与压缩头部的](https://mp.weixin.qq.com/s/7MGmXvC9hnLqsVsEYtmhmQ)
